{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93790554",
   "metadata": {},
   "source": [
    "### Suma Marri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e94c788",
   "metadata": {},
   "source": [
    "#### Problem 1: Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99614915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages to the jupyter notebook\n",
    "# Implement a Linear Regression model using both Normal Equation Method and SGD\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# read and load the csv data file\n",
    "filename = \"Dataset/AMZN.csv\"\n",
    "data = read_csv ( filename )\n",
    "\n",
    "# Get the Adjusted Close Price\n",
    "data_select = data [['Adj Close']]\n",
    "\n",
    "# converting the dataset to a numpy array\n",
    "values = data_select.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22d69b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "\"\"\"\n",
    "Frame a time series as a supervised learning dataset .\n",
    "Arguments :\n",
    "data : Sequence of observations as a list or NumPy array .\n",
    "n_in : Number of lag observations as input (X).\n",
    "n_out : Number of observations as output (y).\n",
    "dropnan : Boolean whether or not to drop rows with NaN values .\n",
    "Returns :\n",
    "Pandas DataFrame of series framed for supervised learning .\n",
    "\"\"\"\n",
    "\n",
    "def series_to_supervised ( data , n_in =1 , n_out =1 , dropnan = True ):\n",
    "    n_vars = 1 if type ( data ) is list else data . shape [1]\n",
    "    df = DataFrame ( data )\n",
    "    cols , names = list () , list ()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range ( n_in , 0 , -1 ):\n",
    "        cols . append ( df . shift ( i ) )\n",
    "        names += [('var %d(t-%d)' % ( j+1 , i ) ) for j in range ( n_vars )]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range (0 , n_out ):\n",
    "        cols.append ( df . shift (-i ) )\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % ( j+1 ) ) for j in range ( n_vars )]\n",
    "        else :\n",
    "            names += [('var%d(t+%d)' % ( j+1 , i ) ) for j in range ( n_vars )]\n",
    "    # put it all together\n",
    "    agg = concat ( cols , axis =1 )\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan :\n",
    "        agg.dropna( inplace = True )\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47d1c62",
   "metadata": {},
   "source": [
    "###### (a) \n",
    "Use the Python function named series_to_supervised() that takes a univariate or multivariate time series and frames it as a supervised learning dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f476cb62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var 1(t-10)</th>\n",
       "      <th>var 1(t-9)</th>\n",
       "      <th>var 1(t-8)</th>\n",
       "      <th>var 1(t-7)</th>\n",
       "      <th>var 1(t-6)</th>\n",
       "      <th>var 1(t-5)</th>\n",
       "      <th>var 1(t-4)</th>\n",
       "      <th>var 1(t-3)</th>\n",
       "      <th>var 1(t-2)</th>\n",
       "      <th>var 1(t-1)</th>\n",
       "      <th>var1(t)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.958333</td>\n",
       "      <td>1.729167</td>\n",
       "      <td>1.708333</td>\n",
       "      <td>1.635417</td>\n",
       "      <td>1.427083</td>\n",
       "      <td>1.395833</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.583333</td>\n",
       "      <td>1.531250</td>\n",
       "      <td>1.505208</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.729167</td>\n",
       "      <td>1.708333</td>\n",
       "      <td>1.635417</td>\n",
       "      <td>1.427083</td>\n",
       "      <td>1.395833</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.583333</td>\n",
       "      <td>1.531250</td>\n",
       "      <td>1.505208</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.510417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.708333</td>\n",
       "      <td>1.635417</td>\n",
       "      <td>1.427083</td>\n",
       "      <td>1.395833</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.583333</td>\n",
       "      <td>1.531250</td>\n",
       "      <td>1.505208</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.510417</td>\n",
       "      <td>1.479167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.635417</td>\n",
       "      <td>1.427083</td>\n",
       "      <td>1.395833</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.583333</td>\n",
       "      <td>1.531250</td>\n",
       "      <td>1.505208</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.510417</td>\n",
       "      <td>1.479167</td>\n",
       "      <td>1.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.427083</td>\n",
       "      <td>1.395833</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.583333</td>\n",
       "      <td>1.531250</td>\n",
       "      <td>1.505208</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.510417</td>\n",
       "      <td>1.479167</td>\n",
       "      <td>1.416667</td>\n",
       "      <td>1.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5753</th>\n",
       "      <td>1676.609985</td>\n",
       "      <td>1785.000000</td>\n",
       "      <td>1689.150024</td>\n",
       "      <td>1807.839966</td>\n",
       "      <td>1830.000000</td>\n",
       "      <td>1880.930054</td>\n",
       "      <td>1846.089966</td>\n",
       "      <td>1902.829956</td>\n",
       "      <td>1940.099976</td>\n",
       "      <td>1885.839966</td>\n",
       "      <td>1955.489990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5754</th>\n",
       "      <td>1785.000000</td>\n",
       "      <td>1689.150024</td>\n",
       "      <td>1807.839966</td>\n",
       "      <td>1830.000000</td>\n",
       "      <td>1880.930054</td>\n",
       "      <td>1846.089966</td>\n",
       "      <td>1902.829956</td>\n",
       "      <td>1940.099976</td>\n",
       "      <td>1885.839966</td>\n",
       "      <td>1955.489990</td>\n",
       "      <td>1900.099976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5755</th>\n",
       "      <td>1689.150024</td>\n",
       "      <td>1807.839966</td>\n",
       "      <td>1830.000000</td>\n",
       "      <td>1880.930054</td>\n",
       "      <td>1846.089966</td>\n",
       "      <td>1902.829956</td>\n",
       "      <td>1940.099976</td>\n",
       "      <td>1885.839966</td>\n",
       "      <td>1955.489990</td>\n",
       "      <td>1900.099976</td>\n",
       "      <td>1963.949951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5756</th>\n",
       "      <td>1807.839966</td>\n",
       "      <td>1830.000000</td>\n",
       "      <td>1880.930054</td>\n",
       "      <td>1846.089966</td>\n",
       "      <td>1902.829956</td>\n",
       "      <td>1940.099976</td>\n",
       "      <td>1885.839966</td>\n",
       "      <td>1955.489990</td>\n",
       "      <td>1900.099976</td>\n",
       "      <td>1963.949951</td>\n",
       "      <td>1949.719971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5757</th>\n",
       "      <td>1830.000000</td>\n",
       "      <td>1880.930054</td>\n",
       "      <td>1846.089966</td>\n",
       "      <td>1902.829956</td>\n",
       "      <td>1940.099976</td>\n",
       "      <td>1885.839966</td>\n",
       "      <td>1955.489990</td>\n",
       "      <td>1900.099976</td>\n",
       "      <td>1963.949951</td>\n",
       "      <td>1949.719971</td>\n",
       "      <td>1907.699951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5748 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      var 1(t-10)   var 1(t-9)   var 1(t-8)   var 1(t-7)   var 1(t-6)  \\\n",
       "10       1.958333     1.729167     1.708333     1.635417     1.427083   \n",
       "11       1.729167     1.708333     1.635417     1.427083     1.395833   \n",
       "12       1.708333     1.635417     1.427083     1.395833     1.500000   \n",
       "13       1.635417     1.427083     1.395833     1.500000     1.583333   \n",
       "14       1.427083     1.395833     1.500000     1.583333     1.531250   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "5753  1676.609985  1785.000000  1689.150024  1807.839966  1830.000000   \n",
       "5754  1785.000000  1689.150024  1807.839966  1830.000000  1880.930054   \n",
       "5755  1689.150024  1807.839966  1830.000000  1880.930054  1846.089966   \n",
       "5756  1807.839966  1830.000000  1880.930054  1846.089966  1902.829956   \n",
       "5757  1830.000000  1880.930054  1846.089966  1902.829956  1940.099976   \n",
       "\n",
       "       var 1(t-5)   var 1(t-4)   var 1(t-3)   var 1(t-2)   var 1(t-1)  \\\n",
       "10       1.395833     1.500000     1.583333     1.531250     1.505208   \n",
       "11       1.500000     1.583333     1.531250     1.505208     1.500000   \n",
       "12       1.583333     1.531250     1.505208     1.500000     1.510417   \n",
       "13       1.531250     1.505208     1.500000     1.510417     1.479167   \n",
       "14       1.505208     1.500000     1.510417     1.479167     1.416667   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "5753  1880.930054  1846.089966  1902.829956  1940.099976  1885.839966   \n",
       "5754  1846.089966  1902.829956  1940.099976  1885.839966  1955.489990   \n",
       "5755  1902.829956  1940.099976  1885.839966  1955.489990  1900.099976   \n",
       "5756  1940.099976  1885.839966  1955.489990  1900.099976  1963.949951   \n",
       "5757  1885.839966  1955.489990  1900.099976  1963.949951  1949.719971   \n",
       "\n",
       "          var1(t)  \n",
       "10       1.500000  \n",
       "11       1.510417  \n",
       "12       1.479167  \n",
       "13       1.416667  \n",
       "14       1.541667  \n",
       "...           ...  \n",
       "5753  1955.489990  \n",
       "5754  1900.099976  \n",
       "5755  1963.949951  \n",
       "5756  1949.719971  \n",
       "5757  1907.699951  \n",
       "\n",
       "[5748 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series_to_supervised(data_select, n_in=10, n_out=1, dropnan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f36eacfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var 1(t-10)</th>\n",
       "      <th>var 1(t-9)</th>\n",
       "      <th>var 1(t-8)</th>\n",
       "      <th>var 1(t-7)</th>\n",
       "      <th>var 1(t-6)</th>\n",
       "      <th>var 1(t-5)</th>\n",
       "      <th>var 1(t-4)</th>\n",
       "      <th>var 1(t-3)</th>\n",
       "      <th>var 1(t-2)</th>\n",
       "      <th>var 1(t-1)</th>\n",
       "      <th>var1(t)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.958333</td>\n",
       "      <td>1.729167</td>\n",
       "      <td>1.708333</td>\n",
       "      <td>1.635417</td>\n",
       "      <td>1.427083</td>\n",
       "      <td>1.395833</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.583333</td>\n",
       "      <td>1.531250</td>\n",
       "      <td>1.505208</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.729167</td>\n",
       "      <td>1.708333</td>\n",
       "      <td>1.635417</td>\n",
       "      <td>1.427083</td>\n",
       "      <td>1.395833</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.583333</td>\n",
       "      <td>1.531250</td>\n",
       "      <td>1.505208</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.510417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.708333</td>\n",
       "      <td>1.635417</td>\n",
       "      <td>1.427083</td>\n",
       "      <td>1.395833</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.583333</td>\n",
       "      <td>1.531250</td>\n",
       "      <td>1.505208</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.510417</td>\n",
       "      <td>1.479167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.635417</td>\n",
       "      <td>1.427083</td>\n",
       "      <td>1.395833</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.583333</td>\n",
       "      <td>1.531250</td>\n",
       "      <td>1.505208</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.510417</td>\n",
       "      <td>1.479167</td>\n",
       "      <td>1.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.427083</td>\n",
       "      <td>1.395833</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.583333</td>\n",
       "      <td>1.531250</td>\n",
       "      <td>1.505208</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.510417</td>\n",
       "      <td>1.479167</td>\n",
       "      <td>1.416667</td>\n",
       "      <td>1.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5753</th>\n",
       "      <td>1676.609985</td>\n",
       "      <td>1785.000000</td>\n",
       "      <td>1689.150024</td>\n",
       "      <td>1807.839966</td>\n",
       "      <td>1830.000000</td>\n",
       "      <td>1880.930054</td>\n",
       "      <td>1846.089966</td>\n",
       "      <td>1902.829956</td>\n",
       "      <td>1940.099976</td>\n",
       "      <td>1885.839966</td>\n",
       "      <td>1955.489990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5754</th>\n",
       "      <td>1785.000000</td>\n",
       "      <td>1689.150024</td>\n",
       "      <td>1807.839966</td>\n",
       "      <td>1830.000000</td>\n",
       "      <td>1880.930054</td>\n",
       "      <td>1846.089966</td>\n",
       "      <td>1902.829956</td>\n",
       "      <td>1940.099976</td>\n",
       "      <td>1885.839966</td>\n",
       "      <td>1955.489990</td>\n",
       "      <td>1900.099976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5755</th>\n",
       "      <td>1689.150024</td>\n",
       "      <td>1807.839966</td>\n",
       "      <td>1830.000000</td>\n",
       "      <td>1880.930054</td>\n",
       "      <td>1846.089966</td>\n",
       "      <td>1902.829956</td>\n",
       "      <td>1940.099976</td>\n",
       "      <td>1885.839966</td>\n",
       "      <td>1955.489990</td>\n",
       "      <td>1900.099976</td>\n",
       "      <td>1963.949951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5756</th>\n",
       "      <td>1807.839966</td>\n",
       "      <td>1830.000000</td>\n",
       "      <td>1880.930054</td>\n",
       "      <td>1846.089966</td>\n",
       "      <td>1902.829956</td>\n",
       "      <td>1940.099976</td>\n",
       "      <td>1885.839966</td>\n",
       "      <td>1955.489990</td>\n",
       "      <td>1900.099976</td>\n",
       "      <td>1963.949951</td>\n",
       "      <td>1949.719971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5757</th>\n",
       "      <td>1830.000000</td>\n",
       "      <td>1880.930054</td>\n",
       "      <td>1846.089966</td>\n",
       "      <td>1902.829956</td>\n",
       "      <td>1940.099976</td>\n",
       "      <td>1885.839966</td>\n",
       "      <td>1955.489990</td>\n",
       "      <td>1900.099976</td>\n",
       "      <td>1963.949951</td>\n",
       "      <td>1949.719971</td>\n",
       "      <td>1907.699951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5748 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      var 1(t-10)   var 1(t-9)   var 1(t-8)   var 1(t-7)   var 1(t-6)  \\\n",
       "10       1.958333     1.729167     1.708333     1.635417     1.427083   \n",
       "11       1.729167     1.708333     1.635417     1.427083     1.395833   \n",
       "12       1.708333     1.635417     1.427083     1.395833     1.500000   \n",
       "13       1.635417     1.427083     1.395833     1.500000     1.583333   \n",
       "14       1.427083     1.395833     1.500000     1.583333     1.531250   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "5753  1676.609985  1785.000000  1689.150024  1807.839966  1830.000000   \n",
       "5754  1785.000000  1689.150024  1807.839966  1830.000000  1880.930054   \n",
       "5755  1689.150024  1807.839966  1830.000000  1880.930054  1846.089966   \n",
       "5756  1807.839966  1830.000000  1880.930054  1846.089966  1902.829956   \n",
       "5757  1830.000000  1880.930054  1846.089966  1902.829956  1940.099976   \n",
       "\n",
       "       var 1(t-5)   var 1(t-4)   var 1(t-3)   var 1(t-2)   var 1(t-1)  \\\n",
       "10       1.395833     1.500000     1.583333     1.531250     1.505208   \n",
       "11       1.500000     1.583333     1.531250     1.505208     1.500000   \n",
       "12       1.583333     1.531250     1.505208     1.500000     1.510417   \n",
       "13       1.531250     1.505208     1.500000     1.510417     1.479167   \n",
       "14       1.505208     1.500000     1.510417     1.479167     1.416667   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "5753  1880.930054  1846.089966  1902.829956  1940.099976  1885.839966   \n",
       "5754  1846.089966  1902.829956  1940.099976  1885.839966  1955.489990   \n",
       "5755  1902.829956  1940.099976  1885.839966  1955.489990  1900.099976   \n",
       "5756  1940.099976  1885.839966  1955.489990  1900.099976  1963.949951   \n",
       "5757  1885.839966  1955.489990  1900.099976  1963.949951  1949.719971   \n",
       "\n",
       "          var1(t)  \n",
       "10       1.500000  \n",
       "11       1.510417  \n",
       "12       1.479167  \n",
       "13       1.416667  \n",
       "14       1.541667  \n",
       "...           ...  \n",
       "5753  1955.489990  \n",
       "5754  1900.099976  \n",
       "5755  1963.949951  \n",
       "5756  1949.719971  \n",
       "5757  1907.699951  \n",
       "\n",
       "[5748 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supervised_data = series_to_supervised(data_select, n_in=10, n_out=1, dropnan=True)\n",
    "supervised_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d90fc0b",
   "metadata": {},
   "source": [
    "###### (b) \n",
    "Use MinMaxScaler to scale your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc6e9088",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "supervised_data = scaler.fit_transform(supervised_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc8afa2",
   "metadata": {},
   "source": [
    "###### (c)\n",
    "Use the Normal Equation Method to find the linear regression coefficients (w). To perform this you may want to take the following steps first: Split your data to X and Y by taking the columns var1(t-10),...,var(t-1) as your 10 features in X, and take the last column var1(t) as your target (Y). Expand your matrix X with a bias vector of ones as the first column (to accomplish this, you may want to use the numpy operations np.ones , np.reshape and np.append). Use the train test split with ‘random state=1’ to split your data to 70% training, and 30% test data. Solve the Normal Equation Method in (2) to find the coefficients w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1413d7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = supervised_data[:,-1]\n",
    "X = supervised_data[:,0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f9300f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5748, 10)\n",
      "(5748,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "752a34c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.58927888e-05 -5.67708993e-02  2.09842519e-01 -1.65870717e-01\n",
      "  2.89934644e-02 -6.11953566e-02  7.25451627e-02 -4.27946096e-02\n",
      "  4.97008003e-03  7.34127990e-02  9.37474158e-01]\n"
     ]
    }
   ],
   "source": [
    "def normalEquation(X, Y):\n",
    "    m = int(np.size(X[:, 1]))\n",
    "    # This is the feature / parameter (2x2) vector that will\n",
    "    # contain my minimized values\n",
    "    theta = []\n",
    "    \n",
    "    # create a bias_vector to add to my newly created X vector\n",
    "    bias_vector = np.ones((m, 1))\n",
    "    \n",
    "    # combine these two vectors together to get a (m, 2) matrix\n",
    "    X = np.append(bias_vector, X, axis=1)\n",
    "    # Normal Equation:\n",
    "    # theta = inv(X^T * X) * X^T * y\n",
    "    \n",
    "    # For convenience create a new, tranposed X matrix\n",
    "    X_transpose = np.transpose(X)\n",
    "    \n",
    "    # Calculating theta\n",
    "    theta = np.linalg.inv(X_transpose.dot(X))\n",
    "    theta = theta.dot(X_transpose)\n",
    "    theta = theta.dot(Y)\n",
    "    \n",
    "    return theta\n",
    "\n",
    "p = normalEquation(X, Y)\n",
    "\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bde4843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.26111054e-04, -4.83348220e-02,  2.24558809e-01, -1.52228741e-01,\n",
       "        3.28901068e-02, -1.08811996e-01,  7.17528590e-02, -2.60760209e-02,\n",
       "       -1.48358090e-03,  8.01298345e-02,  9.28047368e-01])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3,\n",
    "random_state=1)\n",
    "coef = normalEquation(X_train, Y_train)\n",
    "coef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5d1dbf",
   "metadata": {},
   "source": [
    "###### (d)\n",
    "Make a prediction on your test set using the linear regression function f(x) = wT x, and use both the mean square error and coefficient of determination R2 to measure the performance of your prediction model. For this use fucntions mean squared error and r2 score from sklearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf3623cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(row, coefficients):\n",
    "    yhat = coefficients[0] # is bias\n",
    "    for i in range(len(row)):\n",
    "        yhat = yhat + coefficients[i + 1] * row[i] # b+ W * x(inputs - row)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "594b1530",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = np.array(predict(X_test[0,:], coef))\n",
    "for i in range(X_test.shape[0]-1):\n",
    "    Y_pred = np.append(Y_pred, predict(X_test[i+1,:], coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2842ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4148595608723037e-05\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(Y_pred, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22794896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9995582657234232\n"
     ]
    }
   ],
   "source": [
    "print(r2_score(Y_pred, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c613fc45",
   "metadata": {},
   "source": [
    "###### (e)\n",
    "Next, find the coefficients w using gradient descent algorithm and monitor how your error changes in each epoch; You can create a function coefficients sgd similar to what we did in our Lab Session 7. Note that you may have to make some minor changes to this part of the code ( coefficients sgd for linear regression, in lab session 7), due to the additional bias term 1 in your matrix X. For this part, use learning rate 0.01, and number of epochs (iterations) 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ece4617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coefficients_sgd(X_train, Y_train, l_rate, n_epoch): #l-rate is learning rate\n",
    "  #initializing all coefficients to zero\n",
    "  coef = [0.0 for i in range(len(X_train[0])+1)]\n",
    "  for epoch in range(n_epoch):\n",
    "    sum_error = 0 # loss\n",
    "    for i in range(X_train.shape[0]):\n",
    "      # calculating the prediction using current coeeficients\n",
    "      yhat = predict(X_train[i,:], coef)\n",
    "      # calculating error\n",
    "      error = yhat - Y_train[i] #yhat is prediction, Y_train is ground truth,\n",
    "      sum_error += error**2 # error square, because loss cannot be negative, or we want to error to be positive.\n",
    "      #stochastic gradient descent\n",
    "      coef[0] = coef[0] - l_rate * error\n",
    "      for j in range(len(coef)-1):\n",
    "        coef[j + 1] = coef[j + 1] - l_rate * error * X_train[i,j]\n",
    "    \n",
    "    print( ' >epoch=%d, lrate=%.3f, error=%.3f ' % (epoch, l_rate, sum_error))\n",
    "  #returning the list of coefficients  \n",
    "  return coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "384f6b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >epoch=0, lrate=0.010, error=5.666 \n",
      " >epoch=1, lrate=0.010, error=0.453 \n",
      " >epoch=2, lrate=0.010, error=0.447 \n",
      " >epoch=3, lrate=0.010, error=0.442 \n",
      " >epoch=4, lrate=0.010, error=0.437 \n",
      " >epoch=5, lrate=0.010, error=0.432 \n",
      " >epoch=6, lrate=0.010, error=0.427 \n",
      " >epoch=7, lrate=0.010, error=0.422 \n",
      " >epoch=8, lrate=0.010, error=0.417 \n",
      " >epoch=9, lrate=0.010, error=0.413 \n",
      " >epoch=10, lrate=0.010, error=0.408 \n",
      " >epoch=11, lrate=0.010, error=0.404 \n",
      " >epoch=12, lrate=0.010, error=0.399 \n",
      " >epoch=13, lrate=0.010, error=0.395 \n",
      " >epoch=14, lrate=0.010, error=0.391 \n",
      " >epoch=15, lrate=0.010, error=0.387 \n",
      " >epoch=16, lrate=0.010, error=0.383 \n",
      " >epoch=17, lrate=0.010, error=0.379 \n",
      " >epoch=18, lrate=0.010, error=0.375 \n",
      " >epoch=19, lrate=0.010, error=0.371 \n",
      " >epoch=20, lrate=0.010, error=0.367 \n",
      " >epoch=21, lrate=0.010, error=0.364 \n",
      " >epoch=22, lrate=0.010, error=0.360 \n",
      " >epoch=23, lrate=0.010, error=0.357 \n",
      " >epoch=24, lrate=0.010, error=0.353 \n",
      " >epoch=25, lrate=0.010, error=0.350 \n",
      " >epoch=26, lrate=0.010, error=0.347 \n",
      " >epoch=27, lrate=0.010, error=0.343 \n",
      " >epoch=28, lrate=0.010, error=0.340 \n",
      " >epoch=29, lrate=0.010, error=0.337 \n",
      " >epoch=30, lrate=0.010, error=0.334 \n",
      " >epoch=31, lrate=0.010, error=0.331 \n",
      " >epoch=32, lrate=0.010, error=0.328 \n",
      " >epoch=33, lrate=0.010, error=0.326 \n",
      " >epoch=34, lrate=0.010, error=0.323 \n",
      " >epoch=35, lrate=0.010, error=0.320 \n",
      " >epoch=36, lrate=0.010, error=0.317 \n",
      " >epoch=37, lrate=0.010, error=0.315 \n",
      " >epoch=38, lrate=0.010, error=0.312 \n",
      " >epoch=39, lrate=0.010, error=0.310 \n",
      " >epoch=40, lrate=0.010, error=0.307 \n",
      " >epoch=41, lrate=0.010, error=0.305 \n",
      " >epoch=42, lrate=0.010, error=0.302 \n",
      " >epoch=43, lrate=0.010, error=0.300 \n",
      " >epoch=44, lrate=0.010, error=0.298 \n",
      " >epoch=45, lrate=0.010, error=0.296 \n",
      " >epoch=46, lrate=0.010, error=0.293 \n",
      " >epoch=47, lrate=0.010, error=0.291 \n",
      " >epoch=48, lrate=0.010, error=0.289 \n",
      " >epoch=49, lrate=0.010, error=0.287 \n",
      " >epoch=50, lrate=0.010, error=0.285 \n",
      " >epoch=51, lrate=0.010, error=0.283 \n",
      " >epoch=52, lrate=0.010, error=0.281 \n",
      " >epoch=53, lrate=0.010, error=0.279 \n",
      " >epoch=54, lrate=0.010, error=0.277 \n",
      " >epoch=55, lrate=0.010, error=0.275 \n",
      " >epoch=56, lrate=0.010, error=0.274 \n",
      " >epoch=57, lrate=0.010, error=0.272 \n",
      " >epoch=58, lrate=0.010, error=0.270 \n",
      " >epoch=59, lrate=0.010, error=0.268 \n",
      " >epoch=60, lrate=0.010, error=0.267 \n",
      " >epoch=61, lrate=0.010, error=0.265 \n",
      " >epoch=62, lrate=0.010, error=0.264 \n",
      " >epoch=63, lrate=0.010, error=0.262 \n",
      " >epoch=64, lrate=0.010, error=0.260 \n",
      " >epoch=65, lrate=0.010, error=0.259 \n",
      " >epoch=66, lrate=0.010, error=0.257 \n",
      " >epoch=67, lrate=0.010, error=0.256 \n",
      " >epoch=68, lrate=0.010, error=0.255 \n",
      " >epoch=69, lrate=0.010, error=0.253 \n",
      " >epoch=70, lrate=0.010, error=0.252 \n",
      " >epoch=71, lrate=0.010, error=0.250 \n",
      " >epoch=72, lrate=0.010, error=0.249 \n",
      " >epoch=73, lrate=0.010, error=0.248 \n",
      " >epoch=74, lrate=0.010, error=0.246 \n",
      " >epoch=75, lrate=0.010, error=0.245 \n",
      " >epoch=76, lrate=0.010, error=0.244 \n",
      " >epoch=77, lrate=0.010, error=0.243 \n",
      " >epoch=78, lrate=0.010, error=0.242 \n",
      " >epoch=79, lrate=0.010, error=0.240 \n",
      " >epoch=80, lrate=0.010, error=0.239 \n",
      " >epoch=81, lrate=0.010, error=0.238 \n",
      " >epoch=82, lrate=0.010, error=0.237 \n",
      " >epoch=83, lrate=0.010, error=0.236 \n",
      " >epoch=84, lrate=0.010, error=0.235 \n",
      " >epoch=85, lrate=0.010, error=0.234 \n",
      " >epoch=86, lrate=0.010, error=0.233 \n",
      " >epoch=87, lrate=0.010, error=0.232 \n",
      " >epoch=88, lrate=0.010, error=0.231 \n",
      " >epoch=89, lrate=0.010, error=0.230 \n",
      " >epoch=90, lrate=0.010, error=0.229 \n",
      " >epoch=91, lrate=0.010, error=0.228 \n",
      " >epoch=92, lrate=0.010, error=0.227 \n",
      " >epoch=93, lrate=0.010, error=0.226 \n",
      " >epoch=94, lrate=0.010, error=0.225 \n",
      " >epoch=95, lrate=0.010, error=0.224 \n",
      " >epoch=96, lrate=0.010, error=0.223 \n",
      " >epoch=97, lrate=0.010, error=0.223 \n",
      " >epoch=98, lrate=0.010, error=0.222 \n",
      " >epoch=99, lrate=0.010, error=0.221 \n",
      " >epoch=100, lrate=0.010, error=0.220 \n",
      " >epoch=101, lrate=0.010, error=0.219 \n",
      " >epoch=102, lrate=0.010, error=0.218 \n",
      " >epoch=103, lrate=0.010, error=0.218 \n",
      " >epoch=104, lrate=0.010, error=0.217 \n",
      " >epoch=105, lrate=0.010, error=0.216 \n",
      " >epoch=106, lrate=0.010, error=0.215 \n",
      " >epoch=107, lrate=0.010, error=0.215 \n",
      " >epoch=108, lrate=0.010, error=0.214 \n",
      " >epoch=109, lrate=0.010, error=0.213 \n",
      " >epoch=110, lrate=0.010, error=0.213 \n",
      " >epoch=111, lrate=0.010, error=0.212 \n",
      " >epoch=112, lrate=0.010, error=0.211 \n",
      " >epoch=113, lrate=0.010, error=0.211 \n",
      " >epoch=114, lrate=0.010, error=0.210 \n",
      " >epoch=115, lrate=0.010, error=0.209 \n",
      " >epoch=116, lrate=0.010, error=0.209 \n",
      " >epoch=117, lrate=0.010, error=0.208 \n",
      " >epoch=118, lrate=0.010, error=0.208 \n",
      " >epoch=119, lrate=0.010, error=0.207 \n",
      " >epoch=120, lrate=0.010, error=0.206 \n",
      " >epoch=121, lrate=0.010, error=0.206 \n",
      " >epoch=122, lrate=0.010, error=0.205 \n",
      " >epoch=123, lrate=0.010, error=0.205 \n",
      " >epoch=124, lrate=0.010, error=0.204 \n",
      " >epoch=125, lrate=0.010, error=0.204 \n",
      " >epoch=126, lrate=0.010, error=0.203 \n",
      " >epoch=127, lrate=0.010, error=0.203 \n",
      " >epoch=128, lrate=0.010, error=0.202 \n",
      " >epoch=129, lrate=0.010, error=0.201 \n",
      " >epoch=130, lrate=0.010, error=0.201 \n",
      " >epoch=131, lrate=0.010, error=0.200 \n",
      " >epoch=132, lrate=0.010, error=0.200 \n",
      " >epoch=133, lrate=0.010, error=0.200 \n",
      " >epoch=134, lrate=0.010, error=0.199 \n",
      " >epoch=135, lrate=0.010, error=0.199 \n",
      " >epoch=136, lrate=0.010, error=0.198 \n",
      " >epoch=137, lrate=0.010, error=0.198 \n",
      " >epoch=138, lrate=0.010, error=0.197 \n",
      " >epoch=139, lrate=0.010, error=0.197 \n",
      " >epoch=140, lrate=0.010, error=0.196 \n",
      " >epoch=141, lrate=0.010, error=0.196 \n",
      " >epoch=142, lrate=0.010, error=0.195 \n",
      " >epoch=143, lrate=0.010, error=0.195 \n",
      " >epoch=144, lrate=0.010, error=0.195 \n",
      " >epoch=145, lrate=0.010, error=0.194 \n",
      " >epoch=146, lrate=0.010, error=0.194 \n",
      " >epoch=147, lrate=0.010, error=0.193 \n",
      " >epoch=148, lrate=0.010, error=0.193 \n",
      " >epoch=149, lrate=0.010, error=0.193 \n",
      " >epoch=150, lrate=0.010, error=0.192 \n",
      " >epoch=151, lrate=0.010, error=0.192 \n",
      " >epoch=152, lrate=0.010, error=0.192 \n",
      " >epoch=153, lrate=0.010, error=0.191 \n",
      " >epoch=154, lrate=0.010, error=0.191 \n",
      " >epoch=155, lrate=0.010, error=0.190 \n",
      " >epoch=156, lrate=0.010, error=0.190 \n",
      " >epoch=157, lrate=0.010, error=0.190 \n",
      " >epoch=158, lrate=0.010, error=0.189 \n",
      " >epoch=159, lrate=0.010, error=0.189 \n",
      " >epoch=160, lrate=0.010, error=0.189 \n",
      " >epoch=161, lrate=0.010, error=0.188 \n",
      " >epoch=162, lrate=0.010, error=0.188 \n",
      " >epoch=163, lrate=0.010, error=0.188 \n",
      " >epoch=164, lrate=0.010, error=0.187 \n",
      " >epoch=165, lrate=0.010, error=0.187 \n",
      " >epoch=166, lrate=0.010, error=0.187 \n",
      " >epoch=167, lrate=0.010, error=0.187 \n",
      " >epoch=168, lrate=0.010, error=0.186 \n",
      " >epoch=169, lrate=0.010, error=0.186 \n",
      " >epoch=170, lrate=0.010, error=0.186 \n",
      " >epoch=171, lrate=0.010, error=0.185 \n",
      " >epoch=172, lrate=0.010, error=0.185 \n",
      " >epoch=173, lrate=0.010, error=0.185 \n",
      " >epoch=174, lrate=0.010, error=0.184 \n",
      " >epoch=175, lrate=0.010, error=0.184 \n",
      " >epoch=176, lrate=0.010, error=0.184 \n",
      " >epoch=177, lrate=0.010, error=0.184 \n",
      " >epoch=178, lrate=0.010, error=0.183 \n",
      " >epoch=179, lrate=0.010, error=0.183 \n",
      " >epoch=180, lrate=0.010, error=0.183 \n",
      " >epoch=181, lrate=0.010, error=0.183 \n",
      " >epoch=182, lrate=0.010, error=0.182 \n",
      " >epoch=183, lrate=0.010, error=0.182 \n",
      " >epoch=184, lrate=0.010, error=0.182 \n",
      " >epoch=185, lrate=0.010, error=0.182 \n",
      " >epoch=186, lrate=0.010, error=0.181 \n",
      " >epoch=187, lrate=0.010, error=0.181 \n",
      " >epoch=188, lrate=0.010, error=0.181 \n",
      " >epoch=189, lrate=0.010, error=0.181 \n",
      " >epoch=190, lrate=0.010, error=0.180 \n",
      " >epoch=191, lrate=0.010, error=0.180 \n",
      " >epoch=192, lrate=0.010, error=0.180 \n",
      " >epoch=193, lrate=0.010, error=0.180 \n",
      " >epoch=194, lrate=0.010, error=0.179 \n",
      " >epoch=195, lrate=0.010, error=0.179 \n",
      " >epoch=196, lrate=0.010, error=0.179 \n",
      " >epoch=197, lrate=0.010, error=0.179 \n",
      " >epoch=198, lrate=0.010, error=0.179 \n",
      " >epoch=199, lrate=0.010, error=0.178 \n"
     ]
    }
   ],
   "source": [
    "l_rate = 0.01\n",
    "n_epoch = 200\n",
    "coef = coefficients_sgd(X_train, Y_train, l_rate, n_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcc80d0",
   "metadata": {},
   "source": [
    "###### (f)\n",
    "Make a prediction using the coefficients you found from SGD algorithm in previous step (Y prediction sgd = X test.dot(coef sgd)); Use both the mean square error and coefficient of determination R2 to measure the performance of your predictions; compare the results with your prediction performance in part d where you used the coefficients found from Normal Equation Method. Which method gives you better results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5c03f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = np.array(predict(X_test[0,:], coef))\n",
    "for i in range(X_test.shape[0]-1):\n",
    "  Y_pred = np.append(Y_pred, predict(X_test[i+1,:], coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2db894c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5368255527927726e-05\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(Y_pred, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd02e9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9993455884170656\n"
     ]
    }
   ],
   "source": [
    "print(r2_score(Y_pred, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca08119",
   "metadata": {},
   "source": [
    "For R2 score in the normal equation, we get 0.9995582657234232 and the R2 score in SGD is 0.9993455884170656. The MSE for the normal equation is 2.4148595608723037e-05 and the MSE for SGD is 3.5368255527927726e-05.\n",
    "\n",
    "If we comapre the results, there is a very little differnece between Normal Equation and SGD. However, taking that minimal difference into consideration, Normal Equation perform better than SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c430051",
   "metadata": {},
   "source": [
    "#### Problem 2\n",
    "Create a Perceptron model with an optimal value of hyperparameter α (learning rate of SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe3177a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages to the Jupyter notebook\n",
    "# Implement a Perceptron algorithm with an optimal value of learning rate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# read and load the csv data file\n",
    "filename = \"Dataset/sonar.all-data.csv\"\n",
    "dataframe = read_csv (filename)\n",
    "\n",
    "# converting the dataset to a numpy array\n",
    "array = dataframe . values\n",
    "\n",
    "# separate array into input and output components\n",
    "X = array [:,:-1]\n",
    "Y = array [:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff73938",
   "metadata": {},
   "source": [
    "###### (a)\n",
    "Split your data into train and test portions with ‘test size = 0.3’ and ’random state = 3’ . Define your learning model to be Perceptron. Use RepeatedStratifiedKFold with ‘n splits=10’, ‘n repeats=5’, and ‘random state=1’ as your model evaluation method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "688f5222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Perceptron()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=3)\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0bd3934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Perceptron()\n",
    "# define model evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320eb2d7",
   "metadata": {},
   "source": [
    "###### (b)\n",
    "Use GridSearchCV to perform a gird search on the parameter of Perceptron algorithm (learning rate α in SGD), consider values for α as [0.0001, 0.001, 0.01, 0.1]. For your GridSearch, use data only from your training sets (X-train, Y_train)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "edb28487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define grid\n",
    "grid = dict()\n",
    "grid['alpha'] = [0.0001, 0.001, 0.01, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ed78b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define search\n",
    "search = GridSearchCV(model, grid, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# perform the search\n",
    "results = search.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5324a1",
   "metadata": {},
   "source": [
    "###### (c)\n",
    "Report the best score and the best value of the parameter in your search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02be24fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.664\n",
      "Config: {'alpha': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "# summarize\n",
    "print('Mean Accuracy: %.3f' % results.best_score_)\n",
    "print('Config: %s' % results.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5083d00e",
   "metadata": {},
   "source": [
    "###### (d)\n",
    "Create a Perceptron model which takes as an argument the best value of parameter you found in the previous step, and use this model to make predictions on your test set; Report the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "abb8e366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7172413793103448"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Perceptron(alpha=0.0001)\n",
    "results = clf.fit(X_train, Y_train)\n",
    "results.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c609c18",
   "metadata": {},
   "source": [
    "If you see in part c, we used the attribute best_score_ on GridSearchCV to find the mean accuracy or the mean cross-validated score of the best estimator, which was about 0.664. We also used the best_params_ attribute to find the \n",
    "parameter setting that gave the best results on the hold out data, which happened to be 0.0001 in this example. \n",
    "Then, when we used the score() method on the Perceptron model. The score() method returns the mean accuracy on the given test data and labels. We got a mean accuracy of about 0.712, which is higher than the mean accuracy of the GridSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602b5036",
   "metadata": {},
   "source": [
    "#### Problem 3: Create a KNN model with an optimal value of hyperparameter K (the number of nearest neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8889448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages to the Jupyter notebook\n",
    "# Create a KNN model with the best parameter K\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib . pyplot as plt\n",
    "\n",
    "from pandas import read_csv\n",
    "from sklearn . model_selection import train_test_split\n",
    "from sklearn . metrics import accuracy_score\n",
    "from sklearn . neighbors import KNeighborsClassifier\n",
    "\n",
    "# read and load the csv data file\n",
    "filename = \"Dataset/sonar.all-data.csv\"\n",
    "dataframe = read_csv (filename)\n",
    "\n",
    "# converting the dataset to a numpy array\n",
    "array = dataframe . values\n",
    "\n",
    "# separate array into input and output components\n",
    "X = array [:,:-1]\n",
    "Y = array [:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19491c6e",
   "metadata": {},
   "source": [
    "###### (a) \n",
    "Split the data into train and test sets with 'test_size = 0.3', and 'random_state = 5'. Create a KNN model with parameter 'n_neighbor' varying from 1 to 30 (see the code from Lab Session 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efcc7e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fa9b29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "for k in range(1,30):\n",
    "  knn = KNeighborsClassifier(n_neighbors=k)\n",
    "  knn.fit(X_train, Y_train)\n",
    "  y_pred = knn.predict(X_test)\n",
    "  scores[k] = accuracy_score(y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa180769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.7777777777777778,\n",
       " 2: 0.7142857142857143,\n",
       " 3: 0.7301587301587301,\n",
       " 4: 0.7142857142857143,\n",
       " 5: 0.746031746031746,\n",
       " 6: 0.746031746031746,\n",
       " 7: 0.6507936507936508,\n",
       " 8: 0.6349206349206349,\n",
       " 9: 0.6666666666666666,\n",
       " 10: 0.6666666666666666,\n",
       " 11: 0.6825396825396826,\n",
       " 12: 0.6507936507936508,\n",
       " 13: 0.6666666666666666,\n",
       " 14: 0.6507936507936508,\n",
       " 15: 0.6825396825396826,\n",
       " 16: 0.6666666666666666,\n",
       " 17: 0.6507936507936508,\n",
       " 18: 0.6666666666666666,\n",
       " 19: 0.6507936507936508,\n",
       " 20: 0.7142857142857143,\n",
       " 21: 0.6825396825396826,\n",
       " 22: 0.6825396825396826,\n",
       " 23: 0.6984126984126984,\n",
       " 24: 0.6825396825396826,\n",
       " 25: 0.6825396825396826,\n",
       " 26: 0.6825396825396826,\n",
       " 27: 0.6666666666666666,\n",
       " 28: 0.6984126984126984,\n",
       " 29: 0.7142857142857143}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52928df2",
   "metadata": {},
   "source": [
    "###### (b)\n",
    "Plot the accuracy of the KNN model in terms of the number of nearest neighbor k varying from 1 to 30. Choose and report the best value for k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9ccec02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1aad288b5b0>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(scores.keys()),list(scores.values()),marker=\"o\", markersize=2, markeredgecolor=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71abefde",
   "metadata": {},
   "source": [
    "After running the KNeighborsClassifier with diffrent number of neighbors (1 -30), we can see that the best value of k is 1. If you see the list of accuracy classification scores and the line graph, you can see that k=1 has the highest  accuracy. Then 5 and 6 would be the next best values for k. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506280d5",
   "metadata": {},
   "source": [
    "###### (c)\n",
    "Create a new KNN model with the best values of nearest neighbors that you found in previous step, and perform prediction on your test set. Report the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d90a6411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, Y_train)\n",
    "knn.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bca00eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7777777777777778"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = knn.predict(X_test)\n",
    "score = accuracy_score(y_pred, Y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28ee2bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8344827586206897"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, Y_train)\n",
    "knn.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b18844a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.746031746031746"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = knn.predict(X_test)\n",
    "score = accuracy_score(y_pred, Y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe54421a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8275862068965517"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "knn.fit(X_train, Y_train)\n",
    "knn.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84e42adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.746031746031746"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = knn.predict(X_test)\n",
    "score = accuracy_score(y_pred, Y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6b53ce",
   "metadata": {},
   "source": [
    "When I take the KNeighborsClassifier and use the score method, I get the mean accuracy of the data. However, I found this unreliable, because it is checking if it is an exact match of X_train to get high accuracy. That is why the accuracy_score(y_pred, Y_test) was a better test. I choosed the 3 highest accuracy's (k = 1, 5, & 6). k = 1 was the highest and 5/6 tied for 2nd. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da760b73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
