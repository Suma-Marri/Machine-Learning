# In[118]:


import pandas as pd
from pandas import read_csv
from matplotlib import pyplot
from pandas.plotting import autocorrelation_plot
from statsmodels.tsa.stattools import adfuller
from random import randrange
from statsmodels.tsa.seasonal import seasonal_decompose
from sklearn.linear_model import LinearRegression
import numpy
from pandas import DataFrame
from statsmodels.tsa.arima_model import ARIMA
from sklearn.metrics import mean_squared_error
from math import sqrt
filename ='C:/Users/Dell/Downloads/655556_637841654088435073_hw05-datasets/hw05-DataSets/CarSales.csv '
pd1=pd.read_csv(filename,header=0,parse_dates=True,index_col=0,squeeze=True)


# In[110]:


pd3=pd.read_csv(filename,parse_dates=True)
pd3.head(5)
pd3.columns


# In[119]:


import matplotlib.pyplot as plt
plt.plot(pd1)
plt.show()


# In[3]:


autocorrelation_plot(pd1)


# In[4]:


#Augmented Dickey Fuller test
adfuller(pd1)
#We see the p-value is greater then 0.05 so the series is non stationary


# In[5]:


X=seasonal_decompose(pd1,period=4)
print(X.seasonal)


# In[6]:


plt.plot(X.seasonal)
plt.show()


# In[22]:


plt.plot(X.observed)
plt.show()


# In[23]:


plt.plot(X.trend)
plt.show()


# In[24]:


plt.plot(X.resid)
plt.show()


# In[116]:


X1=pd3['Month'].values.tolist()
fg=[]
c=1
for y in X1:
    fg.append(c)
    c+=1
pd3['count']=fg
X=pd3['count'].values.tolist()
y=pd3['Sales']
#pd3.head(5)
reg =LinearRegression().fit(np.array(X).reshape(-1,1), y)
reg.score(np.array(X).reshape(-1,1), y)


# In[130]:


Y2=reg.predict(np.array(X).reshape(-1,1))
import matplotlib.pyplot as plt

figure, axis = plt.subplots(2, 2)
axis[0, 0].plot(X1,Y2)
axis[0, 0].set_title("Linear regression")
  
# For Cosine Function
axis[0, 1].plot(pd1)
axis[0, 1].set_title("Time series")
plt.plot(Y2)
plt.show()


# In[29]:


import numpy as np
import tensorflow as tf
import random as python_random
# fix random seed for reproducibility
def reset_seeds ():
 np.random.seed ( 123 )
 python_random.seed ( 123 )
 tf.random.set_seed ( 1234 )
 reset_seeds ()
from numpy import loadtxt
from keras.models import Sequential
from keras.layers import Dense,InputLayer
from keras.wrappers. scikit_learn import KerasClassifier
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
import numpy
from pandas import read_csv
# load the dataset
filename = "C:/Users/Dell/Downloads/655556_637841654088435073_hw05-datasets/hw05-DataSets/pima-indians-diabetes.csv"
names = ['preg ', 'plas ', 'pres ', 'skin ', 'test ', 'mass ', 'pedi ', 'age ', 'class ']
dataset = read_csv ( filename , names = names )
# split into input (X) and output (y) variables
array = dataset . values
X = array [:,0:8]
y = array [:,8]


# In[71]:


def create_model():
    model = tf.keras.models.Sequential()
    model.add(tf.keras.layers.Dense(12,input_dim=8 ,activation='relu'))

# Now the model will take as input arrays of shape (None, 16)  
# and output arrays of shape (None, 32).  
# Note that after the first layer, you don't need to specify  
# the size of the input anymore:  
    model.add(tf.keras.layers.Dense(8,activation='relu'))
    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
    return model
#m1=create_model(X,y)
m2=KerasClassifier(build_fn=create_model,epochs=10,batch_size=16,verbose=0)    


# In[72]:


print(m2)


# In[79]:


dict1={'epochs':[20,50,70],'batch_size':[16,32]}
clf = GridSearchCV(m2, dict1,cv=4)
'''kfold=KFold(n_splits=4)
cv_results=cross_val_score(clf,X,y,cv=kfold,verbose=1)'''


# In[82]:


fg=clf.fit(np.array(X),np.array(y))


# In[86]:


for t in fg.cv_results_.keys():
    print(t,fg.cv_results_[t])


# In[77]:


dict1={'epochs':[20,50,70],'batch_size':[16,32]}
clf = GridSearchCV(m2, dict1)


# In[94]:


from keras.datasets import cifar10
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import Flatten
from keras.constraints import maxnorm
from keras.optimizers import SGD
from keras.layers.convolutional import Conv2D
from keras.layers.convolutional import MaxPooling2D
from keras.utils import np_utils
from tensorflow.keras.utils import to_categorical
# load data
( X_train,y_train),(X_test ,y_test)=cifar10.load_data ()
# normalize inputs from 0-255 to 0.0-1.0
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train = X_train / 255.0
X_test = X_test / 255.0
# one hot encode outputs
y_train = np_utils.to_categorical ( y_train )
y_test = np_utils.to_categorical ( y_test )
num_classes = y_test.shape [1]


# In[101]:


from keras.layers import Input,Conv2D,MaxPooling2D,UpSampling2D,Flatten,Conv2DTranspose,Concatenate,BatchNormalization
from keras.models import Model, Sequential
from keras.layers.core import Dense, Dropout
from keras.layers.advanced_activations import LeakyReLU
def create_model():
    input1 = Input(shape=(32,32,3 ))
    first_conv = Conv2D(32,(3,3),activation='relu',padding='same')(input1)
    #sec_max=MaxPooling2D((2,2))(first_conv)
    first_conv=(BatchNormalization(momentum=0.8))(first_conv)
    first_conv=Dropout(rate=0.3)(first_conv) # apply 30% dropout to the next layer

    first_conv = Conv2D(32,(3,3),activation='relu',padding='same')(input1)
    #sec_max=MaxPooling2D((2,2))(first_conv)
    first_conv=Dropout(rate=0.3)(first_conv) # apply 30% dropout to the next layer
    first_conv=(BatchNormalization(momentum=0.8))(first_conv)
    sec_max=MaxPooling2D((2,2))(first_conv)
    flat = Flatten()(sec_max)
    dense1=Dense(512,activation='relu')(flat)
    dense2=Dense(10, activation='softmax')(dense1)
    model = Model(inputs=input1, outputs=dense2, name="classify_model")
    model.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])
    return model
mod1=create_model()


# In[102]:


mod1.summary()


# In[103]:


mod1.fit(np.array(X_train),np.array(y_train),epochs=25,batch_size=32)


# In[136]:


x=[1,-1,-1,-1,7]
f=np.array(x)
print(f)
print(np.gradient(f))


# In[ ]:



